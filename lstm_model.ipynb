{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import random\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import gensim\n",
    "from string import punctuation\n",
    "import torch.autograd as autograd\n",
    "import torch.utils.data as Data\n",
    "from torchtext.vocab import Vectors\n",
    "import gensim.downloader as api\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate_test_split(df, train_percent=.6, validate_percent=.2, seed=None):\n",
    "    np.random.seed(seed)\n",
    "    perm = np.random.permutation(df.index)\n",
    "    m = len(df.index)\n",
    "    train_end = int(train_percent * m)\n",
    "    validate_end = int(validate_percent * m) + train_end\n",
    "    train = df.iloc[perm[:train_end]]\n",
    "    validate = df.iloc[perm[train_end:validate_end]]\n",
    "    test = df.iloc[perm[validate_end:]]\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_seq(x):\n",
    "    X = [[w2v[i] for i in sent if i in w2v] for sent in x]\n",
    "    # X = [[i for i in sent if i in w2v] for sent in x]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(x, Y):\n",
    "    x = prepare_seq(list(x))\n",
    "    Y = list(Y)\n",
    "    if(len(x) == 0):\n",
    "        print(\"yep.\")\n",
    "    return [(x, y) for x,y in zip(x, Y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_size(sentences):\n",
    "    l = set()\n",
    "    for i in sentences:\n",
    "        for x in i:\n",
    "            l.add(x)\n",
    "    \n",
    "    return len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_token_to_ix(sentences):\n",
    "    token_to_ix = dict()\n",
    "    print(len(sentences))\n",
    "    for sent in sentences:\n",
    "        for token in sent:# .split(' '):\n",
    "            if token not in token_to_ix:\n",
    "                token_to_ix[token] = len(token_to_ix)\n",
    "    token_to_ix['<pad>'] = len(token_to_ix)\n",
    "    return token_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_label_to_ix(labels):\n",
    "    label_to_ix = dict()\n",
    "    for label in labels:\n",
    "        if label not in label_to_ix:\n",
    "            label_to_ix[label] = len(label_to_ix)\n",
    "    \n",
    "    label_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_infrequent_words(sents):\n",
    "\tword_counts = {}\n",
    "\tfor s in sents:\n",
    "\t\tfor w in s:\n",
    "\t\t\tif w in word_counts:\n",
    "\t\t\t\tword_counts[w] += 1\n",
    "\t\t\telse:\n",
    "\t\t\t\tword_counts[w] = 1\n",
    "\n",
    "\tthreshold = 2\n",
    "\tfiltered_sents = []\n",
    "\tfor s in sents:\n",
    "\t\tnew_s = []\n",
    "\t\tfor w in s:\n",
    "\t\t\tif word_counts[w] < threshold:\n",
    "\t\t\t\tnew_s.append('<UNKOWN>')\n",
    "\t\t\telse:\n",
    "\t\t\t\tnew_s.append(w)\n",
    "\t\tfiltered_sents.append(new_s)\n",
    "\treturn filtered_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restrict_w2v(w2v, restricted_word_set):\n",
    "    new_vectors = []\n",
    "    new_vocab = {}\n",
    "    new_index2entity = []\n",
    "    # new_vectors_norm = []\n",
    "\n",
    "    for i in range(len(w2v.vocab)):\n",
    "        word = w2v.index2entity[i]\n",
    "        vec = w2v.vectors[i]\n",
    "        vocab = w2v.vocab[word]\n",
    "        # vec_norm = w2v.vectors_norm[i]\n",
    "        if word in restricted_word_set:\n",
    "            vocab.index = len(new_index2entity)\n",
    "            new_index2entity.append(word)\n",
    "            new_vocab[word] = vocab\n",
    "            new_vectors.append(vec)\n",
    "            # new_vectors_norm.append(vec_norm)\n",
    "\n",
    "    w2v.vocab = new_vocab\n",
    "    w2v.vectors = new_vectors\n",
    "    w2v.index2entity = new_index2entity\n",
    "    w2v.index2word = new_index2entity\n",
    "    # w2v.vectors_norm = new_vectors_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(truth, pred):\n",
    "    assert len(truth)==len(pred)\n",
    "    right = 0\n",
    "    for i in range(len(truth)):\n",
    "        if truth[i]==pred[i]:\n",
    "            right += 1.0\n",
    "    return right/len(truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "torch.backends.cudnn.fastest = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"processed_request_corpus.csv\").drop(\"Unnamed: 0\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=[\"r_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_text = [t.translate(str.maketrans('', '', string.punctuation)).lower().split() for t in df[\"r_text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_text = remove_infrequent_words(r_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix = build_token_to_ix(r_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_ix = {0:0,1:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = list(df[\"doc_released\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r_text.sort(key=len)\n",
    "# r_text.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_len = [len(r) for r in r_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(r_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split and Embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = api.load('glove-wiki-gigaword-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(w2v.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_df = pd.DataFrame({\"x\":r_text, \"y\":y, \"len\": [len(r) for r in r_text]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_df = len_df[len_df[\"len\"] < 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(len_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = train_validate_test_split(pd.DataFrame({\"x\":list(len_df[\"x\"]), \"y\":list(len_df[\"y\"])}), seed=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data   = prep_data(train[\"x\"], train[\"y\"])\n",
    "dev_data     = prep_data(valid[\"x\"], valid[\"y\"])\n",
    "test_data    = prep_data(test[\"x\"], test[\"y\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These will usually be more like 32 or 64 dimensional.\n",
    "# We will keep them small, so we can see how the weights change as we train.\n",
    "EMBEDDING_DIM = 50\n",
    "HIDDEN_DIM    = 50\n",
    "VOCAB_SIZE    = 40000 # len(word_to_ix)\n",
    "OUTPUT_SIZE   = len(label_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, label_size):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.weights         = torch.FloatTensor(w2v.vectors)\n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(self.weights)\n",
    "        # model.word_embeddings.weight.requires_grad=False\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.hidden2label = nn.Linear(hidden_dim, 1)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # the first is the hidden h\n",
    "        # the second is the cell  c\n",
    "        return (autograd.Variable(torch.zeros(1, 1, self.hidden_dim)),\n",
    "                autograd.Variable(torch.zeros(1, 1, self.hidden_dim)))\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        # embeds = self.word_embeddings(sentence)\n",
    "        # x = embeds.view(len(sentence), 1, -1)\n",
    "        x = sentence\n",
    "        lstm_out, self.hidden = self.lstm(x.view(len(sentence), 1, -1), self.hidden)\n",
    "        y  = self.hidden2label(lstm_out[-1])\n",
    "        log_probs = torch.sigmoid(y)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model         = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, VOCAB_SIZE, OUTPUT_SIZE)\n",
    "model = LSTMClassifier(embedding_dim=EMBEDDING_DIM,hidden_dim=HIDDEN_DIM,\n",
    "                           vocab_size=VOCAB_SIZE,label_size=OUTPUT_SIZE)\n",
    "model.cuda()\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See what the scores are before training\n",
    "# Note that element i,j of the output is the score for tag j for word i.\n",
    "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    # inputs = prepare_sequence(train_data, wv) # my train data is already prepped\n",
    "    tag_scores = model(torch.tensor(train_data[0][0], dtype=torch.float))\n",
    "    print(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.sort(key=lambda x: len(x[0]))\n",
    "train_data.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [(x,y) for x,y in train_data if len(x)>0]\n",
    "train_data.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "loss_l = 0.0\n",
    "loss   = []\n",
    "for epoch in range(80):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    model.train()\n",
    "    for sentence, label in train_data:\n",
    "        if(len(sentence) == 0): # this for zero len sentences, that we cannot seem to filter out???\n",
    "            # print(\"empty sentence\")\n",
    "            continue\n",
    "        #print(tag)\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "        model.hidden = model.init_hidden()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        \n",
    "        sentence = torch.tensor(sentence).type(torch.cuda.FloatTensor)\n",
    "        # Step 3. Run our forward pass.\n",
    "        label_score = model(sentence)\n",
    "        label = torch.tensor([[label]]).type(torch.cuda.FloatTensor)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        # print(pred_label)\n",
    "        # print(label)\n",
    "        loss = loss_function(label_score, label)\n",
    "        loss_l+=loss.item()\n",
    "        # if (i < len(train_data)):\n",
    "        #     loss.backward(retain_graph=True)\n",
    "        # else:\n",
    "        #     print(\"finish epoch\")\n",
    "        #     loss.backward()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        i+=1\n",
    "    # loss.append((loss_l / i))\n",
    "    print(\"epoch \" + str(epoch) + \" \" + str(loss_l / i))\n",
    "    i = 0\n",
    "    loss_l = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See what the scores are before training\n",
    "# Note that element i,j of the output is the score for tag j for word i.\n",
    "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    # inputs = prepare_sequence(train_data, wv) # my train data is already prepped\n",
    "    tag_scores = model(torch.tensor(train_data[0][0], dtype=torch.float))\n",
    "    print(tag_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"mymodel03.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_loss = 0.0\n",
    "truth_res = []\n",
    "pred_res = []\n",
    "\n",
    "for sentence, label in dev_data:\n",
    "    if(len(sentence) == 0): # this for zero len sentences, that we cannot seem to filter out???\n",
    "        continue\n",
    "    truth_res.append(label)\n",
    "    # detaching it from its history on the last instance.\n",
    "    model.hidden = model.init_hidden()\n",
    "    \n",
    "    sentence = torch.tensor(sentence).type(torch.cuda.FloatTensor)\n",
    "    pred = model(sentence)\n",
    "    label = torch.tensor([[label]]).type(torch.cuda.FloatTensor)\n",
    "    if pred.item() > .5:\n",
    "        pred_label = 1\n",
    "    else:\n",
    "        pred_label = 0\n",
    "    pred_res.append(pred_label)\n",
    "    # model.zero_grad() # should I keep this when I am evaluating the model?\n",
    "    loss = loss_function(pred, label)\n",
    "    avg_loss += loss.item()\n",
    "avg_loss /= len(test_data)\n",
    "acc = get_accuracy(truth_res, pred_res)\n",
    "print(' avg_loss:%g train acc:%g' % (avg_loss, acc ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data_labels = [i[1] for i in dev_data if len(i[0]) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_recall_fscore_support(dev_data_labels, pred_res, average=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naivepred_data_labels = [1 for i in dev_data if len(i[0]) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_recall_fscore_support(dev_data_labels, naivepred_data_labels, average=\"binary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valid Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_loss = 0.0\n",
    "truth_res = []\n",
    "pred_res = []\n",
    "\n",
    "for sentence, label in test_data:\n",
    "    if(len(sentence) == 0): # this for zero len sentences, that we cannot seem to filter out???\n",
    "        continue\n",
    "    truth_res.append(label)\n",
    "    # detaching it from its history on the last instance.\n",
    "    model.hidden = model.init_hidden()\n",
    "    \n",
    "    sentence = torch.tensor(sentence).type(torch.cuda.FloatTensor)\n",
    "    pred = model(sentence)\n",
    "    label = torch.tensor([[label]]).type(torch.cuda.FloatTensor)\n",
    "    if pred.item() > .5:\n",
    "        pred_label = 1\n",
    "    else:\n",
    "        pred_label = 0\n",
    "    pred_res.append(pred_label)\n",
    "    # model.zero_grad() # should I keep this when I am evaluating the model?\n",
    "    loss = loss_function(pred, label)\n",
    "    avg_loss += loss.item()\n",
    "avg_loss /= len(test_data)\n",
    "acc = get_accuracy(truth_res, pred_res)\n",
    "print(' avg_loss:%g train acc:%g' % (avg_loss, acc ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_labels = [i[1] for i in test_data if len(i[0]) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_recall_fscore_support(test_data_labels, pred_res, average=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naivepred_data_labels = [1 for i in test_data if len(i[0]) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_recall_fscore_support(test_data_labels, naivepred_data_labels, average=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = get_accuracy(truth_res, pred_res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
