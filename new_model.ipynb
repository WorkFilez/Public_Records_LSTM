{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import random\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import gensim\n",
    "from string import punctuation\n",
    "import torch.autograd as autograd\n",
    "import torch.utils.data as Data\n",
    "from torchtext.vocab import Vectors\n",
    "import gensim.downloader as api\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate_test_split(df, train_percent=.6, validate_percent=.2, seed=None):\n",
    "    np.random.seed(seed)\n",
    "    perm = np.random.permutation(df.index)\n",
    "    m = len(df.index)\n",
    "    train_end = int(train_percent * m)\n",
    "    validate_end = int(validate_percent * m) + train_end\n",
    "    train = df.iloc[perm[:train_end]]\n",
    "    validate = df.iloc[perm[train_end:validate_end]]\n",
    "    test = df.iloc[perm[validate_end:]]\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_seq(x):\n",
    "    X = [[w2v[i] for i in sent if i in w2v] for sent in x]\n",
    "    # X = [[i for i in sent if i in w2v] for sent in x]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(x, Y):\n",
    "    x = prepare_seq(list(x))\n",
    "    Y = list(Y)\n",
    "    if(len(x) == 0):\n",
    "        print(\"yep.\")\n",
    "    return [(x, y) for x,y in zip(x, Y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_size(sentences):\n",
    "    l = set()\n",
    "    for i in sentences:\n",
    "        for x in i:\n",
    "            l.add(x)\n",
    "    \n",
    "    return len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_token_to_ix(sentences):\n",
    "    token_to_ix = dict()\n",
    "    print(len(sentences))\n",
    "    for sent in sentences:\n",
    "        for token in sent:# .split(' '):\n",
    "            if token not in token_to_ix:\n",
    "                token_to_ix[token] = len(token_to_ix)\n",
    "    token_to_ix['<pad>'] = len(token_to_ix)\n",
    "    return token_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_label_to_ix(labels):\n",
    "    label_to_ix = dict()\n",
    "    for label in labels:\n",
    "        if label not in label_to_ix:\n",
    "            label_to_ix[label] = len(label_to_ix)\n",
    "    \n",
    "    label_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_infrequent_words(sents):\n",
    "\tword_counts = {}\n",
    "\tfor s in sents:\n",
    "\t\tfor w in s:\n",
    "\t\t\tif w in word_counts:\n",
    "\t\t\t\tword_counts[w] += 1\n",
    "\t\t\telse:\n",
    "\t\t\t\tword_counts[w] = 1\n",
    "\n",
    "\tthreshold = 2\n",
    "\tfiltered_sents = []\n",
    "\tfor s in sents:\n",
    "\t\tnew_s = []\n",
    "\t\tfor w in s:\n",
    "\t\t\tif word_counts[w] < threshold:\n",
    "\t\t\t\tnew_s.append('<UNKOWN>')\n",
    "\t\t\telse:\n",
    "\t\t\t\tnew_s.append(w)\n",
    "\t\tfiltered_sents.append(new_s)\n",
    "\treturn filtered_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restrict_w2v(w2v, restricted_word_set):\n",
    "    new_vectors = []\n",
    "    new_vocab = {}\n",
    "    new_index2entity = []\n",
    "    # new_vectors_norm = []\n",
    "\n",
    "    for i in range(len(w2v.vocab)):\n",
    "        word = w2v.index2entity[i]\n",
    "        vec = w2v.vectors[i]\n",
    "        vocab = w2v.vocab[word]\n",
    "        # vec_norm = w2v.vectors_norm[i]\n",
    "        if word in restricted_word_set:\n",
    "            vocab.index = len(new_index2entity)\n",
    "            new_index2entity.append(word)\n",
    "            new_vocab[word] = vocab\n",
    "            new_vectors.append(vec)\n",
    "            # new_vectors_norm.append(vec_norm)\n",
    "\n",
    "    w2v.vocab = new_vocab\n",
    "    w2v.vectors = new_vectors\n",
    "    w2v.index2entity = new_index2entity\n",
    "    w2v.index2word = new_index2entity\n",
    "    # w2v.vectors_norm = new_vectors_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(truth, pred):\n",
    "    assert len(truth)==len(pred)\n",
    "    right = 0\n",
    "    for i in range(len(truth)):\n",
    "        if truth[i]==pred[i]:\n",
    "            right += 1.0\n",
    "    return right/len(truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "torch.backends.cudnn.fastest = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"processed_request_corpus.csv\").drop(\"Unnamed: 0\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>r_id</th>\n",
       "      <th>r_text</th>\n",
       "      <th>date_submitted</th>\n",
       "      <th>submission_method</th>\n",
       "      <th>receiving_department</th>\n",
       "      <th>assigned_pro</th>\n",
       "      <th>documents_released_to_requester_details</th>\n",
       "      <th>documents_released_details</th>\n",
       "      <th>request_published</th>\n",
       "      <th>request_closed</th>\n",
       "      <th>documents_released_to_requester</th>\n",
       "      <th>documents_released</th>\n",
       "      <th>department_assignment_details</th>\n",
       "      <th>request_closed_hide</th>\n",
       "      <th>request_reopened</th>\n",
       "      <th>request_opened</th>\n",
       "      <th>department_assignment</th>\n",
       "      <th>close_date</th>\n",
       "      <th>doc_released</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19-5614</td>\n",
       "      <td>This firm is performing a Phase I Environmenta...</td>\n",
       "      <td>November 21, 2019</td>\n",
       "      <td>web</td>\n",
       "      <td>Fire-Rescue</td>\n",
       "      <td>Angela Laurita</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Duplicate request This request was already ent...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Request received via web</td>\n",
       "      <td>Fire-Rescue</td>\n",
       "      <td>Request Closed Public November 21 2019</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      r_id                                             r_text  \\\n",
       "0  19-5614  This firm is performing a Phase I Environmenta...   \n",
       "\n",
       "      date_submitted submission_method receiving_department    assigned_pro  \\\n",
       "0  November 21, 2019               web          Fire-Rescue  Angela Laurita   \n",
       "\n",
       "  documents_released_to_requester_details documents_released_details  \\\n",
       "0                                     NaN                        NaN   \n",
       "\n",
       "   request_published                                     request_closed  \\\n",
       "0                NaN  Duplicate request This request was already ent...   \n",
       "\n",
       "  documents_released_to_requester documents_released  \\\n",
       "0                             NaN                NaN   \n",
       "\n",
       "  department_assignment_details request_closed_hide  request_reopened  \\\n",
       "0                           NaN                 NaN               NaN   \n",
       "\n",
       "             request_opened department_assignment  \\\n",
       "0  Request received via web           Fire-Rescue   \n",
       "\n",
       "                               close_date  doc_released  \n",
       "0  Request Closed Public November 21 2019             0  "
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['r_id', 'r_text', 'date_submitted', 'submission_method', 'receiving_department', 'assigned_pro', 'documents_released_to_requester_details', 'documents_released_details', 'request_published', 'request_closed', 'documents_released_to_requester', 'documents_released', 'department_assignment_details', 'request_closed_hide', 'request_reopened', 'request_opened', 'department_assignment', 'close_date', 'doc_released']\n"
     ]
    }
   ],
   "source": [
    "print(list(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=[\"r_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_text = [t.translate(str.maketrans('', '', string.punctuation)).lower().split() for t in df[\"r_text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_text = remove_infrequent_words(r_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17003\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = build_token_to_ix(r_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_ix = {0:0,1:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = list(df[\"doc_released\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r_text.sort(key=len)\n",
    "# r_text.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_len = [len(r) for r in r_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17003"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(r_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split and Embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = api.load('glove-wiki-gigaword-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w2v.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_df = pd.DataFrame({\"x\":r_text, \"y\":y, \"len\": [len(r) for r in r_text]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_df = len_df[len_df[\"len\"] < 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(len_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = train_validate_test_split(pd.DataFrame({\"x\":list(len_df[\"x\"]), \"y\":list(len_df[\"y\"])}), seed=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data   = prep_data(train[\"x\"], train[\"y\"])\n",
    "dev_data     = prep_data(valid[\"x\"], valid[\"y\"])\n",
    "test_data    = prep_data(test[\"x\"], test[\"y\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These will usually be more like 32 or 64 dimensional.\n",
    "# We will keep them small, so we can see how the weights change as we train.\n",
    "EMBEDDING_DIM = 50\n",
    "HIDDEN_DIM    = 50\n",
    "VOCAB_SIZE    = 40000 # len(word_to_ix)\n",
    "OUTPUT_SIZE   = len(label_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, label_size):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.weights         = torch.FloatTensor(w2v.vectors)\n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(self.weights)\n",
    "        # model.word_embeddings.weight.requires_grad=False\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.hidden2label = nn.Linear(hidden_dim, 1)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # the first is the hidden h\n",
    "        # the second is the cell  c\n",
    "        return (autograd.Variable(torch.zeros(1, 1, self.hidden_dim)),\n",
    "                autograd.Variable(torch.zeros(1, 1, self.hidden_dim)))\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        # embeds = self.word_embeddings(sentence)\n",
    "        # x = embeds.view(len(sentence), 1, -1)\n",
    "        x = sentence\n",
    "        lstm_out, self.hidden = self.lstm(x.view(len(sentence), 1, -1), self.hidden)\n",
    "        y  = self.hidden2label(lstm_out[-1])\n",
    "        log_probs = torch.sigmoid(y)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model         = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, VOCAB_SIZE, OUTPUT_SIZE)\n",
    "model = LSTMClassifier(embedding_dim=EMBEDDING_DIM,hidden_dim=HIDDEN_DIM,\n",
    "                           vocab_size=VOCAB_SIZE,label_size=OUTPUT_SIZE)\n",
    "model.cuda()\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5062]])\n"
     ]
    }
   ],
   "source": [
    "# See what the scores are before training\n",
    "# Note that element i,j of the output is the score for tag j for word i.\n",
    "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    # inputs = prepare_sequence(train_data, wv) # my train data is already prepped\n",
    "    tag_scores = model(torch.tensor(train_data[0][0], dtype=torch.float))\n",
    "    print(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.sort(key=lambda x: len(x[0]))\n",
    "train_data.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7783"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [(x,y) for x,y in train_data if ~(len(x)==0)]\n",
    "train_data.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7783"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 00.6566970421439542\n",
      "epoch 10.6407897275714947\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "loss_l = 0.0\n",
    "loss   = []\n",
    "for epoch in range(80):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    model.train()\n",
    "    for sentence, label in train_data:\n",
    "        if(len(sentence) == 0): # this for zero len sentences, that we cannot seem to filter out???\n",
    "            # print(\"empty sentence\")\n",
    "            continue\n",
    "        #print(tag)\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "        model.hidden = model.init_hidden()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        \n",
    "        sentence = torch.tensor(sentence).type(torch.cuda.FloatTensor)\n",
    "        # Step 3. Run our forward pass.\n",
    "        label_score = model(sentence)\n",
    "        label = torch.tensor([[label]]).type(torch.cuda.FloatTensor)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        # print(pred_label)\n",
    "        # print(label)\n",
    "        loss = loss_function(label_score, label)\n",
    "        loss_l+=loss.item()\n",
    "        # if (i < len(train_data)):\n",
    "        #     loss.backward(retain_graph=True)\n",
    "        # else:\n",
    "        #     print(\"finish epoch\")\n",
    "        #     loss.backward()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        i+=1\n",
    "    # loss.append((loss_l / i))\n",
    "    print(\"epoch \" + str(epoch) + str(loss_l / i))\n",
    "    i = 0\n",
    "    loss_l = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000]])\n"
     ]
    }
   ],
   "source": [
    "# See what the scores are before training\n",
    "# Note that element i,j of the output is the score for tag j for word i.\n",
    "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    # inputs = prepare_sequence(train_data, wv) # my train data is already prepped\n",
    "    tag_scores = model(torch.tensor(train_data[0][0], dtype=torch.float))\n",
    "    print(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"mymodel01.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empty sentence\n",
      "empty sentence\n",
      "empty sentence\n",
      "empty sentence\n",
      "empty sentence\n",
      "empty sentence\n",
      "empty sentence\n",
      "empty sentence\n",
      "empty sentence\n",
      "empty sentence\n",
      "empty sentence\n",
      "empty sentence\n",
      "empty sentence\n",
      "empty sentence\n",
      "empty sentence\n",
      "empty sentence\n",
      "empty sentence\n",
      "empty sentence\n",
      "empty sentence\n",
      "empty sentence\n",
      "empty sentence\n",
      "empty sentence\n",
      "empty sentence\n",
      "empty sentence\n",
      "empty sentence\n",
      "empty sentence\n",
      "empty sentence\n",
      "empty sentence\n",
      "empty sentence\n",
      "empty sentence\n",
      "empty sentence\n",
      "empty sentence\n",
      "empty sentence\n",
      "empty sentence\n",
      "empty sentence\n",
      "empty sentence\n",
      "empty sentence\n",
      "empty sentence\n",
      "empty sentence\n",
      "empty sentence\n",
      "empty sentence\n",
      "empty sentence\n",
      " avg_loss:2.01276 train acc:0.604577\n"
     ]
    }
   ],
   "source": [
    "avg_loss = 0.0\n",
    "truth_res = []\n",
    "pred_res = []\n",
    "\n",
    "for sentence, label in test_data:\n",
    "    if(len(sentence) == 0): # this for zero len sentences, that we cannot seem to filter out???\n",
    "        print(\"empty sentence\")\n",
    "        continue\n",
    "    truth_res.append(label)\n",
    "    # detaching it from its history on the last instance.\n",
    "    model.hidden = model.init_hidden()\n",
    "    \n",
    "    sentence = torch.tensor(sentence).type(torch.cuda.FloatTensor)\n",
    "    pred = model(sentence)\n",
    "    label = torch.tensor([[label]]).type(torch.cuda.FloatTensor)\n",
    "    if pred.item() > .5:\n",
    "        pred_label = 1\n",
    "    else:\n",
    "        pred_label = 0\n",
    "    pred_res.append(pred_label)\n",
    "    # model.zero_grad() # should I keep this when I am evaluating the model?\n",
    "    loss = loss_function(pred, label)\n",
    "    avg_loss += loss.item()\n",
    "avg_loss /= len(test_data)\n",
    "acc = get_accuracy(truth_res, pred_res)\n",
    "print(' avg_loss:%g train acc:%g' % (avg_loss, acc ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
