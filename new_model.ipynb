{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import random\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import gensim\n",
    "from string import punctuation\n",
    "import torch.autograd as autograd\n",
    "import torch.utils.data as Data\n",
    "from torchtext.vocab import Vectors\n",
    "import gensim.downloader as api\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate_test_split(df, train_percent=.6, validate_percent=.2, seed=None):\n",
    "    np.random.seed(seed)\n",
    "    perm = np.random.permutation(df.index)\n",
    "    m = len(df.index)\n",
    "    train_end = int(train_percent * m)\n",
    "    validate_end = int(validate_percent * m) + train_end\n",
    "    train = df.iloc[perm[:train_end]]\n",
    "    validate = df.iloc[perm[train_end:validate_end]]\n",
    "    test = df.iloc[perm[validate_end:]]\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_seq(x):\n",
    "    X = [[w2v[i] for i in sent if i in w2v] for sent in x]\n",
    "    # X = [[i for i in sent if i in w2v] for sent in x]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(x, Y):\n",
    "    x = prepare_seq(list(x))\n",
    "    Y = list(Y)\n",
    "    return [(x, y) for x,y in zip(x, Y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_size(sentences):\n",
    "    l = set()\n",
    "    for i in sentences:\n",
    "        for x in i:\n",
    "            l.add(x)\n",
    "    \n",
    "    return len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_token_to_ix(sentences):\n",
    "    token_to_ix = dict()\n",
    "    print(len(sentences))\n",
    "    for sent in sentences:\n",
    "        for token in sent:# .split(' '):\n",
    "            if token not in token_to_ix:\n",
    "                token_to_ix[token] = len(token_to_ix)\n",
    "    token_to_ix['<pad>'] = len(token_to_ix)\n",
    "    return token_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_label_to_ix(labels):\n",
    "    label_to_ix = dict()\n",
    "    for label in labels:\n",
    "        if label not in label_to_ix:\n",
    "            label_to_ix[label] = len(label_to_ix)\n",
    "    \n",
    "    label_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_infrequent_words(sents):\n",
    "\tword_counts = {}\n",
    "\tfor s in sents:\n",
    "\t\tfor w in s:\n",
    "\t\t\tif w in word_counts:\n",
    "\t\t\t\tword_counts[w] += 1\n",
    "\t\t\telse:\n",
    "\t\t\t\tword_counts[w] = 1\n",
    "\n",
    "\tthreshold = 2\n",
    "\tfiltered_sents = []\n",
    "\tfor s in sents:\n",
    "\t\tnew_s = []\n",
    "\t\tfor w in s:\n",
    "\t\t\tif word_counts[w] < threshold:\n",
    "\t\t\t\tnew_s.append('<UNKOWN>')\n",
    "\t\t\telse:\n",
    "\t\t\t\tnew_s.append(w)\n",
    "\t\tfiltered_sents.append(new_s)\n",
    "\treturn filtered_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restrict_w2v(w2v, restricted_word_set):\n",
    "    new_vectors = []\n",
    "    new_vocab = {}\n",
    "    new_index2entity = []\n",
    "    # new_vectors_norm = []\n",
    "\n",
    "    for i in range(len(w2v.vocab)):\n",
    "        word = w2v.index2entity[i]\n",
    "        vec = w2v.vectors[i]\n",
    "        vocab = w2v.vocab[word]\n",
    "        # vec_norm = w2v.vectors_norm[i]\n",
    "        if word in restricted_word_set:\n",
    "            vocab.index = len(new_index2entity)\n",
    "            new_index2entity.append(word)\n",
    "            new_vocab[word] = vocab\n",
    "            new_vectors.append(vec)\n",
    "            # new_vectors_norm.append(vec_norm)\n",
    "\n",
    "    w2v.vocab = new_vocab\n",
    "    w2v.vectors = new_vectors\n",
    "    w2v.index2entity = new_index2entity\n",
    "    w2v.index2word = new_index2entity\n",
    "    # w2v.vectors_norm = new_vectors_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"processed_request_corpus.csv\").drop(\"Unnamed: 0\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>r_id</th>\n",
       "      <th>r_text</th>\n",
       "      <th>date_submitted</th>\n",
       "      <th>submission_method</th>\n",
       "      <th>receiving_department</th>\n",
       "      <th>assigned_pro</th>\n",
       "      <th>documents_released_to_requester_details</th>\n",
       "      <th>documents_released_details</th>\n",
       "      <th>request_published</th>\n",
       "      <th>request_closed</th>\n",
       "      <th>documents_released_to_requester</th>\n",
       "      <th>documents_released</th>\n",
       "      <th>department_assignment_details</th>\n",
       "      <th>request_closed_hide</th>\n",
       "      <th>request_reopened</th>\n",
       "      <th>request_opened</th>\n",
       "      <th>department_assignment</th>\n",
       "      <th>close_date</th>\n",
       "      <th>doc_released</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19-5614</td>\n",
       "      <td>This firm is performing a Phase I Environmenta...</td>\n",
       "      <td>November 21, 2019</td>\n",
       "      <td>web</td>\n",
       "      <td>Fire-Rescue</td>\n",
       "      <td>Angela Laurita</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Duplicate request This request was already ent...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Request received via web</td>\n",
       "      <td>Fire-Rescue</td>\n",
       "      <td>Request Closed Public November 21 2019</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      r_id                                             r_text  \\\n",
       "0  19-5614  This firm is performing a Phase I Environmenta...   \n",
       "\n",
       "      date_submitted submission_method receiving_department    assigned_pro  \\\n",
       "0  November 21, 2019               web          Fire-Rescue  Angela Laurita   \n",
       "\n",
       "  documents_released_to_requester_details documents_released_details  \\\n",
       "0                                     NaN                        NaN   \n",
       "\n",
       "   request_published                                     request_closed  \\\n",
       "0                NaN  Duplicate request This request was already ent...   \n",
       "\n",
       "  documents_released_to_requester documents_released  \\\n",
       "0                             NaN                NaN   \n",
       "\n",
       "  department_assignment_details request_closed_hide  request_reopened  \\\n",
       "0                           NaN                 NaN               NaN   \n",
       "\n",
       "             request_opened department_assignment  \\\n",
       "0  Request received via web           Fire-Rescue   \n",
       "\n",
       "                               close_date  doc_released  \n",
       "0  Request Closed Public November 21 2019             0  "
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['r_id', 'r_text', 'date_submitted', 'submission_method', 'receiving_department', 'assigned_pro', 'documents_released_to_requester_details', 'documents_released_details', 'request_published', 'request_closed', 'documents_released_to_requester', 'documents_released', 'department_assignment_details', 'request_closed_hide', 'request_reopened', 'request_opened', 'department_assignment', 'close_date', 'doc_released']\n"
     ]
    }
   ],
   "source": [
    "print(list(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"r_text\"] = df[\"r_text\"].fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_text = [t.translate(str.maketrans('', '', string.punctuation)).lower().split() for t in df[\"r_text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_text = remove_infrequent_words(r_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17006\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = build_token_to_ix(r_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_ix = {0:0,1:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = list(df[\"doc_released\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r_text.sort(key=len)\n",
    "# r_text.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_len = [len(r) for r in r_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17006"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(r_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split and Embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = api.load('glove-wiki-gigaword-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w2v.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_df = pd.DataFrame({\"x\":r_text, \"y\":y, \"len\": [len(r) for r in r_text]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_df = len_df[len_df[\"len\"] < 51]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9608"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(len_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = train_validate_test_split(pd.DataFrame({\"x\":list(len_df[\"x\"]), \"y\":list(len_df[\"y\"])}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data   = prep_data(train[\"x\"], train[\"y\"])\n",
    "dev_data     = prep_data(valid[\"x\"], valid[\"y\"])\n",
    "test_data    = prep_data(test[\"x\"], test[\"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-9.4531e-01,  3.9686e-01, -8.0605e-01, -3.0215e-01,  2.7736e-01,\n",
       "       -1.0019e-01, -4.0500e-01, -1.0095e-01, -6.5934e-02, -4.7258e-02,\n",
       "       -2.0828e-01, -2.5721e-01,  6.8750e-02,  9.3751e-01, -8.1483e-02,\n",
       "        1.3460e-01,  2.7302e-02, -1.8096e-01, -3.5638e-01, -8.8104e-01,\n",
       "        1.1951e+00,  5.5556e-02, -3.1741e-01,  1.0244e+00, -8.4768e-01,\n",
       "       -1.5959e+00,  2.1657e-02,  4.3628e-01,  8.8388e-04, -4.1820e-01,\n",
       "        2.1247e+00, -4.3332e-01, -1.0816e+00,  3.3616e-01,  3.3399e-01,\n",
       "       -2.0064e-01,  5.8633e-01,  9.0186e-02,  7.5054e-01,  4.8500e-01,\n",
       "        1.7370e-01,  6.8129e-01, -1.6810e-01,  6.1265e-01,  7.6875e-02,\n",
       "       -1.9797e-01, -9.9555e-02, -1.0231e+00,  9.5394e-01, -6.3500e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These will usually be more like 32 or 64 dimensional.\n",
    "# We will keep them small, so we can see how the weights change as we train.\n",
    "EMBEDDING_DIM = 50\n",
    "HIDDEN_DIM    = 50\n",
    "VOCAB_SIZE    = 40000 # len(word_to_ix)\n",
    "OUTPUT_SIZE   = len(label_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.weights         = torch.FloatTensor(w2v.vectors)\n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(self.weights)\n",
    "        model.word_embeddings.weight.requires_grad=False\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        # sentence = torch.abs(sentence)\n",
    "        # embeds = self.word_embeddings(sentence)\n",
    "        embeds      = sentence\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        # print(len(lstm_out[-1][0]))\n",
    "        # tag_space   = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_space   = self.hidden2tag(lstm_out[-1])\n",
    "        tag_scores  = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "model         = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, VOCAB_SIZE, OUTPUT_SIZE)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer     = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(300):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for sentence, tag in train_data:\n",
    "        ntag = [[0,0]]\n",
    "        #print(sentence)\n",
    "        #print(tag)\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        \n",
    "        sentence = torch.tensor(sentence)\n",
    "        ntag[0][tag] = 1\n",
    "        tag      = torch.tensor(ntag)\n",
    "        print(tag)\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model(sentence)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(tag_scores, tag)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10203"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These will usually be more like 32 or 64 dimensional.\n",
    "# We will keep them small, so we can see how the weights change as we train.\n",
    "EMBEDDING_DIM = 50\n",
    "HIDDEN_DIM    = 50\n",
    "VOCAB_SIZE    = 40000 # len(word_to_ix)\n",
    "OUTPUT_SIZE   = len(label_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, label_size):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.weights         = torch.FloatTensor(w2v.vectors)\n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(self.weights)\n",
    "        model.word_embeddings.weight.requires_grad=False\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.hidden2label = nn.Linear(hidden_dim, 1)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # the first is the hidden h\n",
    "        # the second is the cell  c\n",
    "        return (autograd.Variable(torch.zeros(1, 1, self.hidden_dim)),\n",
    "                autograd.Variable(torch.zeros(1, 1, self.hidden_dim)))\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        # embeds = self.word_embeddings(sentence)\n",
    "        # x = embeds.view(len(sentence), 1, -1)\n",
    "        x = sentence\n",
    "        lstm_out, self.hidden = self.lstm(x.view(len(sentence), 1, -1), self.hidden)\n",
    "        y  = self.hidden2label(lstm_out[-1])\n",
    "        log_probs = torch.sigmoid(y)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model         = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, VOCAB_SIZE, OUTPUT_SIZE)\n",
    "model = LSTMClassifier(embedding_dim=EMBEDDING_DIM,hidden_dim=HIDDEN_DIM,\n",
    "                           vocab_size=VOCAB_SIZE,label_size=OUTPUT_SIZE)\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5086]])\n"
     ]
    }
   ],
   "source": [
    "# See what the scores are before training\n",
    "# Note that element i,j of the output is the score for tag j for word i.\n",
    "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    # inputs = prepare_sequence(train_data, wv) # my train data is already prepped\n",
    "    tag_scores = model(torch.tensor(train_data[0][0], dtype=torch.float))\n",
    "    print(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "print(tag_scores.data.max(1)[1].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_l = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.sort(key=lambda x: len(x[0]))\n",
    "train_data.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(20):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for sentence, label in train_data:\n",
    "        # print(len(sentence))\n",
    "        #print(tag)\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        \n",
    "        sentence = torch.tensor(sentence)\n",
    "        # Step 3. Run our forward pass.\n",
    "        label_score = model(sentence)\n",
    "        label = torch.FloatTensor([[label]])\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        # print(pred_label)\n",
    "        # print(label)\n",
    "        loss = loss_function(label_score, label)\n",
    "        # print(loss)\n",
    "        loss_l.append(loss)\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "    print (loss_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
